tests\library\test_sai_model_spec.py:
"""Tests for sai_model_spec module."""

import pytest
import time

from library import sai_model_spec


class MockArgs:
    """Mock argparse.Namespace for testing."""

    def __init__(self, **kwargs):
        # Default values
        self.v2 = False
        self.v_parameterization = False
        self.resolution = 512
        self.metadata_title = None
        self.metadata_author = None
        self.metadata_description = None
        self.metadata_license = None
        self.metadata_tags = None
        self.min_timestep = None
        self.max_timestep = None
        self.clip_skip = None
        self.output_name = "test_output"

        # Override with provided values
        for key, value in kwargs.items():
            setattr(self, key, value)


class TestModelSpecMetadata:
    """Test the ModelSpecMetadata dataclass."""

    def test_creation_and_conversion(self):
        """Test creating dataclass and converting to metadata dict."""
        metadata = sai_model_spec.ModelSpecMetadata(
            architecture="stable-diffusion-v1",
            implementation="diffusers",
            title="Test Model",
            resolution="512x512",
            author="Test Author",
            description=None,  # Test None exclusion
        )

        assert metadata.architecture == "stable-diffusion-v1"
        assert metadata.sai_model_spec == "1.0.1"

        metadata_dict = metadata.to_metadata_dict()
        assert "modelspec.architecture" in metadata_dict
        assert "modelspec.author" in metadata_dict
        assert "modelspec.description" not in metadata_dict  # None values excluded
        assert metadata_dict["modelspec.sai_model_spec"] == "1.0.1"

    def test_additional_fields_handling(self):
        """Test handling of additional metadata fields."""
        additional = {"custom_field": "custom_value", "modelspec.prefixed": "prefixed_value"}

        metadata = sai_model_spec.ModelSpecMetadata(
            architecture="stable-diffusion-v1",
            implementation="diffusers",
            title="Test Model",
            resolution="512x512",
            additional_fields=additional,
        )

        metadata_dict = metadata.to_metadata_dict()
        assert "modelspec.custom_field" in metadata_dict
        assert "modelspec.prefixed" in metadata_dict
        assert metadata_dict["modelspec.custom_field"] == "custom_value"

    def test_from_args_extraction(self):
        """Test creating ModelSpecMetadata from args with metadata_* fields."""
        args = MockArgs(metadata_author="Test Author", metadata_trigger_phrase="anime style", metadata_usage_hint="Use CFG 7.5")

        metadata = sai_model_spec.ModelSpecMetadata.from_args(
            args,
            architecture="stable-diffusion-v1",
            implementation="diffusers",
            title="Test Model",
            resolution="512x512",
        )

        assert metadata.author == "Test Author"
        assert metadata.additional_fields["trigger_phrase"] == "anime style"
        assert metadata.additional_fields["usage_hint"] == "Use CFG 7.5"


class TestArchitectureDetection:
    """Test architecture detection for different model types."""

    @pytest.mark.parametrize(
        "config,expected",
        [
            ({"v2": False, "v_parameterization": False, "sdxl": True}, "stable-diffusion-xl-v1-base"),
            ({"v2": False, "v_parameterization": False, "sdxl": False, "model_config": {"flux": "dev"}}, "flux-1-dev"),
            ({"v2": False, "v_parameterization": False, "sdxl": False, "model_config": {"flux": "chroma"}}, "chroma"),
            (
                {"v2": False, "v_parameterization": False, "sdxl": False, "model_config": {"sd3": "large"}},
                "stable-diffusion-3-large",
            ),
            ({"v2": True, "v_parameterization": True, "sdxl": False}, "stable-diffusion-v2-768-v"),
            ({"v2": False, "v_parameterization": False, "sdxl": False}, "stable-diffusion-v1"),
        ],
    )
    def test_architecture_detection(self, config, expected):
        """Test architecture detection for various model configurations."""
        model_config = config.pop("model_config", None)
        arch = sai_model_spec.determine_architecture(lora=False, textual_inversion=False, model_config=model_config, **config)
        assert arch == expected

    def test_adapter_suffixes(self):
        """Test LoRA and textual inversion suffixes."""
        lora_arch = sai_model_spec.determine_architecture(
            v2=False, v_parameterization=False, sdxl=True, lora=True, textual_inversion=False
        )
        assert lora_arch == "stable-diffusion-xl-v1-base/lora"

        ti_arch = sai_model_spec.determine_architecture(
            v2=False, v_parameterization=False, sdxl=False, lora=False, textual_inversion=True
        )
        assert ti_arch == "stable-diffusion-v1/textual-inversion"


class TestImplementationDetection:
    """Test implementation detection for different model types."""

    @pytest.mark.parametrize(
        "config,expected",
        [
            ({"model_config": {"flux": "dev"}}, "https://github.com/black-forest-labs/flux"),
            ({"model_config": {"flux": "chroma"}}, "https://huggingface.co/lodestones/Chroma"),
            ({"model_config": {"lumina": "lumina2"}}, "https://github.com/Alpha-VLLM/Lumina-Image-2.0"),
            ({"lora": True, "sdxl": True}, "https://github.com/Stability-AI/generative-models"),
            ({"lora": True, "sdxl": False}, "diffusers"),
        ],
    )
    def test_implementation_detection(self, config, expected):
        """Test implementation detection for various configurations."""
        model_config = config.pop("model_config", None)
        impl = sai_model_spec.determine_implementation(
            lora=config.get("lora", False), textual_inversion=False, sdxl=config.get("sdxl", False), model_config=model_config
        )
        assert impl == expected


class TestResolutionHandling:
    """Test resolution parsing and defaults."""

    @pytest.mark.parametrize(
        "input_reso,expected",
        [
            ((768, 1024), "768x1024"),
            (768, "768x768"),
            ("768,1024", "768x1024"),
        ],
    )
    def test_explicit_resolution_formats(self, input_reso, expected):
        """Test different resolution input formats."""
        res = sai_model_spec.determine_resolution(reso=input_reso)
        assert res == expected

    @pytest.mark.parametrize(
        "config,expected",
        [
            ({"sdxl": True}, "1024x1024"),
            ({"model_config": {"flux": "dev"}}, "1024x1024"),
            ({"v2": True, "v_parameterization": True}, "768x768"),
            ({}, "512x512"),  # Default SD v1
        ],
    )
    def test_default_resolutions(self, config, expected):
        """Test default resolution detection."""
        model_config = config.pop("model_config", None)
        res = sai_model_spec.determine_resolution(model_config=model_config, **config)
        assert res == expected


class TestThumbnailProcessing:
    """Test thumbnail data URL processing."""

    def test_file_to_data_url(self):
        """Test converting file to data URL."""
        import tempfile
        import os

        # Create a tiny test PNG (1x1 pixel)
        test_png_data = b"\x89PNG\r\n\x1a\n\x00\x00\x00\rIHDR\x00\x00\x00\x01\x00\x00\x00\x01\x08\x02\x00\x00\x00\x90wS\xde\x00\x00\x00\x0cIDATx\x9cc\xff\xff\xff\x00\x00\x00\x04\x00\x01\x9d\xb3\xa7c\x00\x00\x00\x00IEND\xaeB`\x82"

        with tempfile.NamedTemporaryFile(suffix=".png", delete=False) as f:
            f.write(test_png_data)
            temp_path = f.name

        try:
            data_url = sai_model_spec.file_to_data_url(temp_path)

            # Check format
            assert data_url.startswith("data:image/png;base64,")

            # Check it's a reasonable length (base64 encoded)
            assert len(data_url) > 50

            # Verify we can decode it back
            import base64

            encoded_part = data_url.split(",", 1)[1]
            decoded_data = base64.b64decode(encoded_part)
            assert decoded_data == test_png_data

        finally:
            os.unlink(temp_path)

    def test_file_to_data_url_nonexistent_file(self):
        """Test error handling for nonexistent files."""
        import pytest

        with pytest.raises(FileNotFoundError):
            sai_model_spec.file_to_data_url("/nonexistent/file.png")

    def test_thumbnail_processing_in_metadata(self):
        """Test thumbnail processing in build_metadata_dataclass."""
        import tempfile
        import os

        # Create a test image file
        test_png_data = b"\x89PNG\r\n\x1a\n\x00\x00\x00\rIHDR\x00\x00\x00\x01\x00\x00\x00\x01\x08\x02\x00\x00\x00\x90wS\xde\x00\x00\x00\x0cIDATx\x9cc\xff\xff\xff\x00\x00\x00\x04\x00\x01\x9d\xb3\xa7c\x00\x00\x00\x00IEND\xaeB`\x82"

        with tempfile.NamedTemporaryFile(suffix=".png", delete=False) as f:
            f.write(test_png_data)
            temp_path = f.name

        try:
            timestamp = time.time()

            # Test with file path - should be converted to data URL
            metadata = sai_model_spec.build_metadata_dataclass(
                state_dict=None,
                v2=False,
                v_parameterization=False,
                sdxl=False,
                lora=False,
                textual_inversion=False,
                timestamp=timestamp,
                title="Test Model",
                optional_metadata={"thumbnail": temp_path},
            )

            # Should be converted to data URL
            assert "thumbnail" in metadata.additional_fields
            assert metadata.additional_fields["thumbnail"].startswith("data:image/png;base64,")

        finally:
            os.unlink(temp_path)

    def test_thumbnail_data_url_passthrough(self):
        """Test that existing data URLs are passed through unchanged."""
        timestamp = time.time()

        existing_data_url = (
            "data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mP8/5+hHgAHggJ/PchI7wAAAABJRU5ErkJggg=="
        )

        metadata = sai_model_spec.build_metadata_dataclass(
            state_dict=None,
            v2=False,
            v_parameterization=False,
            sdxl=False,
            lora=False,
            textual_inversion=False,
            timestamp=timestamp,
            title="Test Model",
            optional_metadata={"thumbnail": existing_data_url},
        )

        # Should be unchanged
        assert metadata.additional_fields["thumbnail"] == existing_data_url

    def test_thumbnail_invalid_file_handling(self):
        """Test graceful handling of invalid thumbnail files."""
        timestamp = time.time()

        metadata = sai_model_spec.build_metadata_dataclass(
            state_dict=None,
            v2=False,
            v_parameterization=False,
            sdxl=False,
            lora=False,
            textual_inversion=False,
            timestamp=timestamp,
            title="Test Model",
            optional_metadata={"thumbnail": "/nonexistent/file.png"},
        )

        # Should be removed from additional_fields due to error
        assert "thumbnail" not in metadata.additional_fields


class TestBuildMetadataIntegration:
    """Test the complete metadata building workflow."""

    def test_sdxl_model_workflow(self):
        """Test complete workflow for SDXL model."""
        timestamp = time.time()

        metadata = sai_model_spec.build_metadata_dataclass(
            state_dict=None,
            v2=False,
            v_parameterization=False,
            sdxl=True,
            lora=False,
            textual_inversion=False,
            timestamp=timestamp,
            title="Test SDXL Model",
        )

        assert metadata.architecture == "stable-diffusion-xl-v1-base"
        assert metadata.implementation == "https://github.com/Stability-AI/generative-models"
        assert metadata.resolution == "1024x1024"
        assert metadata.prediction_type == "epsilon"

    def test_flux_model_workflow(self):
        """Test complete workflow for Flux model."""
        timestamp = time.time()

        metadata = sai_model_spec.build_metadata_dataclass(
            state_dict=None,
            v2=False,
            v_parameterization=False,
            sdxl=False,
            lora=False,
            textual_inversion=False,
            timestamp=timestamp,
            title="Test Flux Model",
            model_config={"flux": "dev"},
            optional_metadata={"trigger_phrase": "anime style"},
        )

        assert metadata.architecture == "flux-1-dev"
        assert metadata.implementation == "https://github.com/black-forest-labs/flux"
        assert metadata.prediction_type is None  # Flux doesn't use prediction_type
        assert metadata.additional_fields["trigger_phrase"] == "anime style"

    def test_legacy_function_compatibility(self):
        """Test that legacy build_metadata function works correctly."""
        timestamp = time.time()

        metadata_dict = sai_model_spec.build_metadata(
            state_dict=None,
            v2=False,
            v_parameterization=False,
            sdxl=True,
            lora=False,
            textual_inversion=False,
            timestamp=timestamp,
            title="Test Model",
        )

        assert isinstance(metadata_dict, dict)
        assert metadata_dict["modelspec.sai_model_spec"] == "1.0.1"
        assert metadata_dict["modelspec.architecture"] == "stable-diffusion-xl-v1-base"


tests\library\test_strategy_lumina.py:
import os
import tempfile
import torch
import numpy as np
from unittest.mock import patch
from transformers import Gemma2Model

from library.strategy_lumina import (
    LuminaTokenizeStrategy,
    LuminaTextEncodingStrategy,
    LuminaTextEncoderOutputsCachingStrategy,
    LuminaLatentsCachingStrategy,
)


class SimpleMockGemma2Model:
    """Lightweight mock that avoids initializing the actual Gemma2Model"""

    def __init__(self, hidden_size=2304):
        self.device = torch.device("cpu")
        self._hidden_size = hidden_size
        self._orig_mod = self  # For dynamic compilation compatibility

    def __call__(self, input_ids, attention_mask, output_hidden_states=False, return_dict=False):
        # Create a mock output object with hidden states
        batch_size, seq_len = input_ids.shape
        hidden_size = self._hidden_size

        class MockOutput:
            def __init__(self, hidden_states):
                self.hidden_states = hidden_states

        mock_hidden_states = [
            torch.randn(batch_size, seq_len, hidden_size, device=input_ids.device)
            for _ in range(3)  # Mimic multiple layers of hidden states
        ]

        return MockOutput(mock_hidden_states)


def test_lumina_tokenize_strategy():
    # Test default initialization
    try:
        tokenize_strategy = LuminaTokenizeStrategy("dummy system prompt", max_length=None)
    except OSError as e:
        # If the tokenizer is not found (due to gated repo), we can skip the test
        print(f"Skipping LuminaTokenizeStrategy test due to OSError: {e}")
        return
    assert tokenize_strategy.max_length == 256
    assert tokenize_strategy.tokenizer.padding_side == "right"

    # Test tokenization of a single string
    text = "Hello"
    tokens, attention_mask = tokenize_strategy.tokenize(text)

    assert tokens.ndim == 2
    assert attention_mask.ndim == 2
    assert tokens.shape == attention_mask.shape
    assert tokens.shape[1] == 256  # max_length

    # Test tokenize_with_weights
    tokens, attention_mask, weights = tokenize_strategy.tokenize_with_weights(text)
    assert len(weights) == 1
    assert torch.all(weights[0] == 1)


def test_lumina_text_encoding_strategy():
    # Create strategies
    try:
        tokenize_strategy = LuminaTokenizeStrategy("dummy system prompt", max_length=None)
    except OSError as e:
        # If the tokenizer is not found (due to gated repo), we can skip the test
        print(f"Skipping LuminaTokenizeStrategy test due to OSError: {e}")
        return
    encoding_strategy = LuminaTextEncodingStrategy()

    # Create a mock model
    mock_model = SimpleMockGemma2Model()

    # Patch the isinstance check to accept our simple mock
    original_isinstance = isinstance
    with patch("library.strategy_lumina.isinstance") as mock_isinstance:

        def custom_isinstance(obj, class_or_tuple):
            if obj is mock_model and class_or_tuple is Gemma2Model:
                return True
            if hasattr(obj, "_orig_mod") and obj._orig_mod is mock_model and class_or_tuple is Gemma2Model:
                return True
            return original_isinstance(obj, class_or_tuple)

        mock_isinstance.side_effect = custom_isinstance

        # Prepare sample text
        text = "Test encoding strategy"
        tokens, attention_mask = tokenize_strategy.tokenize(text)

        # Perform encoding
        hidden_states, input_ids, attention_masks = encoding_strategy.encode_tokens(
            tokenize_strategy, [mock_model], (tokens, attention_mask)
        )

        # Validate outputs
        assert original_isinstance(hidden_states, torch.Tensor)
        assert original_isinstance(input_ids, torch.Tensor)
        assert original_isinstance(attention_masks, torch.Tensor)

        # Check the shape of the second-to-last hidden state
        assert hidden_states.ndim == 3

        # Test weighted encoding (which falls back to standard encoding for Lumina)
        weights = [torch.ones_like(tokens)]
        hidden_states_w, input_ids_w, attention_masks_w = encoding_strategy.encode_tokens_with_weights(
            tokenize_strategy, [mock_model], (tokens, attention_mask), weights
        )

        # For the mock, we can't guarantee identical outputs since each call returns random tensors
        # Instead, check that the outputs have the same shape and are tensors
        assert hidden_states_w.shape == hidden_states.shape
        assert original_isinstance(hidden_states_w, torch.Tensor)
        assert torch.allclose(input_ids, input_ids_w)  # Input IDs should be the same
        assert torch.allclose(attention_masks, attention_masks_w)  # Attention masks should be the same


def test_lumina_text_encoder_outputs_caching_strategy():
    # Create a temporary directory for caching
    with tempfile.TemporaryDirectory() as tmpdir:
        # Create a cache file path
        cache_file = os.path.join(tmpdir, "test_outputs.npz")

        # Create the caching strategy
        caching_strategy = LuminaTextEncoderOutputsCachingStrategy(
            cache_to_disk=True,
            batch_size=1,
            skip_disk_cache_validity_check=False,
        )

        # Create a mock class for ImageInfo
        class MockImageInfo:
            def __init__(self, caption, cache_path):
                self.caption = caption
                self.text_encoder_outputs_npz = cache_path

        # Create a sample input info
        image_info = MockImageInfo("Test caption", cache_file)

        # Simulate a batch
        batch = [image_info]

        # Create mock strategies and model
        try:
            tokenize_strategy = LuminaTokenizeStrategy("dummy system prompt", max_length=None)
        except OSError as e:
            # If the tokenizer is not found (due to gated repo), we can skip the test
            print(f"Skipping LuminaTokenizeStrategy test due to OSError: {e}")
            return
        encoding_strategy = LuminaTextEncodingStrategy()
        mock_model = SimpleMockGemma2Model()

        # Patch the isinstance check to accept our simple mock
        original_isinstance = isinstance
        with patch("library.strategy_lumina.isinstance") as mock_isinstance:

            def custom_isinstance(obj, class_or_tuple):
                if obj is mock_model and class_or_tuple is Gemma2Model:
                    return True
                if hasattr(obj, "_orig_mod") and obj._orig_mod is mock_model and class_or_tuple is Gemma2Model:
                    return True
                return original_isinstance(obj, class_or_tuple)

            mock_isinstance.side_effect = custom_isinstance

            # Call cache_batch_outputs
            caching_strategy.cache_batch_outputs(tokenize_strategy, [mock_model], encoding_strategy, batch)

        # Verify the npz file was created
        assert os.path.exists(cache_file), f"Cache file not created at {cache_file}"

        # Verify the is_disk_cached_outputs_expected method
        assert caching_strategy.is_disk_cached_outputs_expected(cache_file)

        # Test loading from npz
        loaded_data = caching_strategy.load_outputs_npz(cache_file)
        assert len(loaded_data) == 3  # hidden_state, input_ids, attention_mask


def test_lumina_latents_caching_strategy():
    # Create a temporary directory for caching
    with tempfile.TemporaryDirectory() as tmpdir:
        # Prepare a mock absolute path
        abs_path = os.path.join(tmpdir, "test_image.png")

        # Use smaller image size for faster testing
        image_size = (64, 64)

        # Create a smaller dummy image for testing
        test_image = np.random.randint(0, 255, (64, 64, 3), dtype=np.uint8)

        # Create the caching strategy
        caching_strategy = LuminaLatentsCachingStrategy(cache_to_disk=True, batch_size=1, skip_disk_cache_validity_check=False)

        # Create a simple mock VAE
        class MockVAE:
            def __init__(self):
                self.device = torch.device("cpu")
                self.dtype = torch.float32

            def encode(self, x):
                # Return smaller encoded tensor for faster processing
                encoded = torch.randn(1, 4, 8, 8, device=x.device)
                return type("EncodedLatents", (), {"to": lambda *args, **kwargs: encoded})

        # Prepare a mock batch
        class MockImageInfo:
            def __init__(self, path, image):
                self.absolute_path = path
                self.image = image
                self.image_path = path
                self.bucket_reso = image_size
                self.resized_size = image_size
                self.resize_interpolation = "lanczos"
                # Specify full path to the latents npz file
                self.latents_npz = os.path.join(tmpdir, f"{os.path.splitext(os.path.basename(path))[0]}_0064x0064_lumina.npz")

        batch = [MockImageInfo(abs_path, test_image)]

        # Call cache_batch_latents
        mock_vae = MockVAE()
        caching_strategy.cache_batch_latents(mock_vae, batch, flip_aug=False, alpha_mask=False, random_crop=False)

        # Generate the expected npz path
        npz_path = caching_strategy.get_latents_npz_path(abs_path, image_size)

        # Verify the file was created
        assert os.path.exists(npz_path), f"NPZ file not created at {npz_path}"

        # Verify is_disk_cached_latents_expected
        assert caching_strategy.is_disk_cached_latents_expected(image_size, npz_path, False, False)

        # Test loading from disk
        loaded_data = caching_strategy.load_latents_from_disk(npz_path, image_size)
        assert len(loaded_data) == 5  # Check for 5 expected elements


tests\test_custom_offloading_utils.py:
import pytest
import torch
import torch.nn as nn
from unittest.mock import patch, MagicMock

from library.custom_offloading_utils import (
    _synchronize_device, 
    swap_weight_devices_cuda,
    swap_weight_devices_no_cuda,
    weighs_to_device,
    Offloader,
    ModelOffloader
)

class TransformerBlock(nn.Module):
    def __init__(self, block_idx: int):
        super().__init__()
        self.block_idx = block_idx
        self.linear1 = nn.Linear(10, 5)
        self.linear2 = nn.Linear(5, 10)
        self.seq = nn.Sequential(nn.SiLU(), nn.Linear(10, 10))
    
    def forward(self, x):
        x = self.linear1(x)
        x = torch.relu(x)
        x = self.linear2(x)
        x = self.seq(x)
        return x


class SimpleModel(nn.Module):
    def __init__(self, num_blocks=16):
        super().__init__()
        self.blocks = nn.ModuleList([
            TransformerBlock(i)
        for i in range(num_blocks)])
    
    def forward(self, x):
        for block in self.blocks:
            x = block(x)
        return x

    @property
    def device(self):
        return next(self.parameters()).device


# Device Synchronization Tests
@patch('torch.cuda.synchronize')
@pytest.mark.skipif(not torch.cuda.is_available(), reason="CUDA not available")
def test_cuda_synchronize(mock_cuda_sync):
    device = torch.device('cuda')
    _synchronize_device(device)
    mock_cuda_sync.assert_called_once()

@patch('torch.xpu.synchronize')
@pytest.mark.skipif(not torch.xpu.is_available(), reason="XPU not available")
def test_xpu_synchronize(mock_xpu_sync):
    device = torch.device('xpu')
    _synchronize_device(device)
    mock_xpu_sync.assert_called_once()

@patch('torch.mps.synchronize')
@pytest.mark.skipif(not torch.xpu.is_available(), reason="MPS not available")
def test_mps_synchronize(mock_mps_sync):
    device = torch.device('mps')
    _synchronize_device(device)
    mock_mps_sync.assert_called_once()


# Weights to Device Tests
def test_weights_to_device():
    # Create a simple model with weights
    model = nn.Sequential(
        nn.Linear(10, 5),
        nn.ReLU(),
        nn.Linear(5, 2)
    )
    
    # Start with CPU tensors
    device = torch.device('cpu')
    for module in model.modules():
        if hasattr(module, "weight") and module.weight is not None:
            assert module.weight.device == device
    
    # Move to mock CUDA device
    mock_device = torch.device('cuda')
    with patch('torch.Tensor.to', return_value=torch.zeros(1).to(device)):
        weighs_to_device(model, mock_device)
        
        # Since we mocked the to() function, we can only verify modules were processed
        # but can't check actual device movement


# Swap Weight Devices Tests
@pytest.mark.skipif(not torch.cuda.is_available(), reason="CUDA not available")
def test_swap_weight_devices_cuda():
    device = torch.device('cuda')
    layer_to_cpu = SimpleModel()
    layer_to_cuda = SimpleModel()

    # Move layer to CUDA to move to CPU
    layer_to_cpu.to(device)
    
    with patch('torch.Tensor.to', return_value=torch.zeros(1)):
        with patch('torch.Tensor.copy_'):
            swap_weight_devices_cuda(device, layer_to_cpu, layer_to_cuda)
            
            assert layer_to_cpu.device.type == 'cpu'
            assert layer_to_cuda.device.type == 'cuda'



@patch('library.custom_offloading_utils._synchronize_device')
def test_swap_weight_devices_no_cuda(mock_sync_device):
    device = torch.device('cpu')
    layer_to_cpu = SimpleModel()
    layer_to_cuda = SimpleModel()
    
    with patch('torch.Tensor.to', return_value=torch.zeros(1)):
        with patch('torch.Tensor.copy_'):
            swap_weight_devices_no_cuda(device, layer_to_cpu, layer_to_cuda)
            
            # Verify _synchronize_device was called twice
            assert mock_sync_device.call_count == 2


# Offloader Tests
@pytest.fixture
def offloader():
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    return Offloader(
        num_blocks=4,
        blocks_to_swap=2,
        device=device,
        debug=False
    )


def test_offloader_init(offloader):
    assert offloader.num_blocks == 4
    assert offloader.blocks_to_swap == 2
    assert hasattr(offloader, 'thread_pool')
    assert offloader.futures == {}
    assert offloader.cuda_available == (offloader.device.type == 'cuda')


@patch('library.custom_offloading_utils.swap_weight_devices_cuda')
@patch('library.custom_offloading_utils.swap_weight_devices_no_cuda')
def test_swap_weight_devices(mock_no_cuda, mock_cuda, offloader: Offloader):
    block_to_cpu = SimpleModel()
    block_to_cuda = SimpleModel()
    
    # Force test for CUDA device
    offloader.cuda_available = True
    offloader.swap_weight_devices(block_to_cpu, block_to_cuda)
    mock_cuda.assert_called_once_with(offloader.device, block_to_cpu, block_to_cuda)
    mock_no_cuda.assert_not_called()
    
    # Reset mocks
    mock_cuda.reset_mock()
    mock_no_cuda.reset_mock()
    
    # Force test for non-CUDA device
    offloader.cuda_available = False
    offloader.swap_weight_devices(block_to_cpu, block_to_cuda)
    mock_no_cuda.assert_called_once_with(offloader.device, block_to_cpu, block_to_cuda)
    mock_cuda.assert_not_called()


@patch('library.custom_offloading_utils.Offloader.swap_weight_devices')
def test_submit_move_blocks(mock_swap, offloader):
    blocks = [SimpleModel() for _ in range(4)]
    block_idx_to_cpu = 0
    block_idx_to_cuda = 2
    
    # Mock the thread pool to execute synchronously
    future = MagicMock()
    future.result.return_value = (block_idx_to_cpu, block_idx_to_cuda)
    offloader.thread_pool.submit = MagicMock(return_value=future)
    
    offloader._submit_move_blocks(blocks, block_idx_to_cpu, block_idx_to_cuda)
    
    # Check that the future is stored with the correct key
    assert block_idx_to_cuda in offloader.futures


def test_wait_blocks_move(offloader):
    block_idx = 2
    
    # Test with no future for the block
    offloader._wait_blocks_move(block_idx)  # Should not raise
    
    # Create a fake future and test waiting
    future = MagicMock()
    future.result.return_value = (0, block_idx)
    offloader.futures[block_idx] = future
    
    offloader._wait_blocks_move(block_idx)
    
    # Check that the future was removed
    assert block_idx not in offloader.futures
    future.result.assert_called_once()


# ModelOffloader Tests
@pytest.fixture
def model_offloader():
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    blocks_to_swap = 2
    blocks = SimpleModel(4).blocks
    return ModelOffloader(
        blocks=blocks,
        blocks_to_swap=blocks_to_swap,
        device=device,
        debug=False
    )


def test_model_offloader_init(model_offloader):
    assert model_offloader.num_blocks == 4
    assert model_offloader.blocks_to_swap == 2
    assert hasattr(model_offloader, 'thread_pool')
    assert model_offloader.futures == {}
    assert len(model_offloader.remove_handles) > 0  # Should have registered hooks


def test_create_backward_hook():
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    blocks_to_swap = 2
    blocks = SimpleModel(4).blocks
    model_offloader = ModelOffloader(
        blocks=blocks,
        blocks_to_swap=blocks_to_swap,
        device=device,
        debug=False
    )

    # Test hook creation for swapping case (block 0)
    hook_swap = model_offloader.create_backward_hook(blocks, 0)
    assert hook_swap is None
    
    # Test hook creation for waiting case (block 1)
    hook_wait = model_offloader.create_backward_hook(blocks, 1)
    assert hook_wait is not None
    
    # Test hook creation for no action case (block 3)
    hook_none = model_offloader.create_backward_hook(blocks, 3)
    assert hook_none is None


@patch('library.custom_offloading_utils.ModelOffloader._submit_move_blocks')
@patch('library.custom_offloading_utils.ModelOffloader._wait_blocks_move')
def test_backward_hook_execution(mock_wait, mock_submit):
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    blocks_to_swap = 2
    model = SimpleModel(4)
    blocks = model.blocks
    model_offloader = ModelOffloader(
        blocks=blocks,
        blocks_to_swap=blocks_to_swap,
        device=device,
        debug=False
    )
    
    # Test swapping hook (block 1)
    hook_swap = model_offloader.create_backward_hook(blocks, 1)
    assert hook_swap is not None
    hook_swap(model, torch.zeros(1), torch.zeros(1))
    mock_submit.assert_called_once()
    
    mock_submit.reset_mock()
    
    # Test waiting hook (block 2)
    hook_wait = model_offloader.create_backward_hook(blocks, 2)
    assert hook_wait is not None
    hook_wait(model, torch.zeros(1), torch.zeros(1))
    assert mock_wait.call_count == 2


@patch('library.custom_offloading_utils.weighs_to_device')
@patch('library.custom_offloading_utils._synchronize_device')
@patch('library.custom_offloading_utils._clean_memory_on_device')
def test_prepare_block_devices_before_forward(mock_clean, mock_sync, mock_weights_to_device, model_offloader):
    model = SimpleModel(4)
    blocks = model.blocks
    
    with patch.object(nn.Module, 'to'):
        model_offloader.prepare_block_devices_before_forward(blocks)
        
        # Check that weighs_to_device was called for each block
        assert mock_weights_to_device.call_count == 4
        
        # Check that _synchronize_device and _clean_memory_on_device were called
        mock_sync.assert_called_once_with(model_offloader.device)
        mock_clean.assert_called_once_with(model_offloader.device)


@patch('library.custom_offloading_utils.ModelOffloader._wait_blocks_move')
def test_wait_for_block(mock_wait, model_offloader):
    # Test with blocks_to_swap=0
    model_offloader.blocks_to_swap = 0
    model_offloader.wait_for_block(1)
    mock_wait.assert_not_called()
    
    # Test with blocks_to_swap=2
    model_offloader.blocks_to_swap = 2
    block_idx = 1
    model_offloader.wait_for_block(block_idx)
    mock_wait.assert_called_once_with(block_idx)


@patch('library.custom_offloading_utils.ModelOffloader._submit_move_blocks')
def test_submit_move_blocks(mock_submit, model_offloader):
    model = SimpleModel()
    blocks = model.blocks
    
    # Test with blocks_to_swap=0
    model_offloader.blocks_to_swap = 0
    model_offloader.submit_move_blocks(blocks, 1)
    mock_submit.assert_not_called()
    
    mock_submit.reset_mock()
    model_offloader.blocks_to_swap = 2
    
    # Test within swap range
    block_idx = 1
    model_offloader.submit_move_blocks(blocks, block_idx)
    mock_submit.assert_called_once()
    
    mock_submit.reset_mock()
    
    # Test outside swap range
    block_idx = 3
    model_offloader.submit_move_blocks(blocks, block_idx)
    mock_submit.assert_not_called()


# Integration test for offloading in a realistic scenario
@pytest.mark.skipif(not torch.cuda.is_available(), reason="CUDA not available")
def test_offloading_integration():
    device = torch.device('cuda')
    # Create a mini model with 4 blocks
    model = SimpleModel(5)
    model.to(device)
    blocks = model.blocks
    
    # Initialize model offloader
    offloader = ModelOffloader(
        blocks=blocks,
        blocks_to_swap=2,
        device=device,
        debug=True
    )
    
    # Prepare blocks for forward pass
    offloader.prepare_block_devices_before_forward(blocks)
    
    # Simulate forward pass with offloading
    input_tensor = torch.randn(1, 10, device=device)
    x = input_tensor
    
    for i, block in enumerate(blocks):
        # Wait for the current block to be ready
        offloader.wait_for_block(i)
        
        # Process through the block
        x = block(x)
        
        # Schedule moving weights for future blocks
        offloader.submit_move_blocks(blocks, i)
    
    # Verify we get a valid output
    assert x.shape == (1, 10)
    assert not torch.isnan(x).any()


# Error handling tests
def test_offloader_assertion_error():
    with pytest.raises(AssertionError):
        device = torch.device('cpu')
        layer_to_cpu = SimpleModel()
        layer_to_cuda = nn.Linear(10, 5)  # Different class
        swap_weight_devices_cuda(device, layer_to_cpu, layer_to_cuda)

if __name__ == "__main__":
    # Run all tests when file is executed directly
    import sys
    
    # Configure pytest command line arguments
    pytest_args = [
        "-v",                   # Verbose output
        "--color=yes",          # Colored output
        __file__,               # Run tests in this file
    ]
    
    # Add optional arguments from command line
    if len(sys.argv) > 1:
        pytest_args.extend(sys.argv[1:])
    
    # Print info about test execution
    print(f"Running tests with PyTorch {torch.__version__}")
    print(f"CUDA available: {torch.cuda.is_available()}")
    if torch.cuda.is_available():
        print(f"CUDA device: {torch.cuda.get_device_name(0)}")
    
    # Run the tests
    sys.exit(pytest.main(pytest_args))


tests\test_fine_tune.py:
import fine_tune


def test_syntax():
    # Very simply testing that the train_network imports without syntax errors
    assert True


